---
title: "P8105 Homework 6"
author: "Malika Top (mt3775)"
date: "2024-12-02"
output: github_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(dplyr)
library(rnoaa)
library(glmnet)
library(modelr)
library(mgcv)
```
## Problem 1

#### Loading data
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```
#### Performing bootstrap and extracting estimates
```{r}
weather_bootstrap =
  weather_df |> 
  modelr::bootstrap(n = 5000) |> 
  mutate(
    models = map(strap, \(df) lm(tmax ~ tmin, data = df)),
    results = map(models, broom::tidy),
    summary = map(models, broom::glance),
    r_squared = map_dbl(summary, \(x) x$r.squared),
    log_prod = map_dbl(results, \(x) log(prod(x$estimate)))
  ) |> 
  select(-models, -summary, -strap) |> 
  unnest(results)
```
#### Distribution of estimates
```{r}
weather_bootstrap |> 
  ggplot(aes(x = r_squared)) +
  geom_density()
```

The distribution of the $\hat{r}^{2}$ estimates looks almost symmetrical with a little
bit of a shoulder on the left. It suggests that most of the bootstrap samples
yielded a $\hat{r}^{2}$ value of around 0.91, which means 91% of the 
variance in `tmax` can be explained by `tmin`. 
```{r}
weather_bootstrap |> 
  ggplot(aes(x = log_prod)) +
  geom_density()
```

The distribution of the log($\hat{\beta_{0}}*\hat{\beta_{1}}$) estimates is also 
fairly symmetrical. 

#### Confidence intervals
```{r}
weather_bootstrap |> 
  group_by(term) |> 
  filter(term == "tmin") |> 
  summarize(
    r_sq_ci_lower = quantile(r_squared, 0.025),
    r_sq_ci_upper = quantile(r_squared, 0.975),
    log_prod_ci_lower = quantile(log_prod, 0.025),
    log_prod_ci_upper = quantile(log_prod, 0.975),
  )
```

## Problem 2
#### Reading in and cleaning data
```{r}
homicide_df = read.csv("data/homicide-data.csv")
homicide_df = homicide_df |> 
  mutate(
    state = case_when(
      state == "wI" ~ "WI",
      TRUE ~ state
    )
  ) |> 
  unite("city_state", city:state, remove = FALSE) |> 
  mutate(
    #status = ifelse(disposition == "Closed by arrest", 1, 0),
    resolved = as.numeric(disposition == "Closed by arrest"),
    victim_age = as.numeric(victim_age),
    victim_race = fct_relevel(victim_race, "White"),
    victim_sex = as.factor(victim_sex)
  ) 
homicide_df_sub =
  homicide_df |> 
  filter(! city_state %in% c("Dallas_TX", "Phoenix_AZ", "Kansas City_MO", "Tulsa_AL")) |> 
  filter(victim_race == "White" | victim_race == "Black")
```
#### Fitting logistic regression model
```{r}
balt_md = 
  homicide_df_sub |> 
  filter(city_state == "Baltimore_MD")
log_reg = 
  balt_md |> 
  glm(resolved ~ victim_age + victim_sex + victim_race, data = _, family = binomial()) 
balt_md_results =
  log_reg |>  
  broom::tidy() |> 
  mutate(OR = exp(estimate), 
         lower_bound = exp(confint(log_reg, "victim_sexMale"))[1],
         upper_bound = exp(confint(log_reg, "victim_sexMale"))[2]
         ) |> 
  filter(term == "victim_sexMale") |> 
  select(term, log_OR = estimate, OR, p.value, lower_bound, upper_bound) 
balt_md_results |> 
  knitr::kable(digits=3)
```
#### GLM for all cities
```{r}
glm_results = 
  homicide_df_sub |> 
  nest(data = -city_state) |> 
  mutate(
    models = map(data, \(df) glm(resolved ~ victim_age + victim_sex + victim_race, data = df, family = binomial())),
    results = map(models, \(x) broom::tidy(x, conf.int = TRUE))
  ) |> 
  unnest(results) |> 
  filter(term == "victim_sexMale") |> 
  mutate(
    OR = exp(estimate), 
    lower_bound = exp(conf.low),
    upper_bound = exp(conf.high)
  ) |> 
  select(city_state, OR, lower_bound, upper_bound, p.value)
glm_results
```
#### Plot of estimated ORs and CIs 
```{r}
glm_results |> 
  mutate(city_state = fct_reorder(city_state, OR)) |> 
  ggplot(aes(x = city_state, y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = lower_bound, ymax = upper_bound)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(title = "Adjusted Odds Ratio for Solved Homicides across U.S")
```

Keeping all other variables fixed, the cities with OR greater than 1 are 
Nashville, TN, Fresno, CA, Stockton, CA, and Albuquerque, NM. This means that for these places,
homicides where the victim was male were more likely to be solved than for female victims. 
For Richmond, VA, and Atlanta, GA, with OR practically equal to 1, that means the odds of the 
homicide being solved was practically same regardless of gender. 

New York, NY has the lowest
OR, so the odds of the homicide being solved is lower for cases where the victim was female. 
However, it is interesting that the confidence intervals are widest for the `city_state` variables
that have the higher ORs, and New York's is fairly narrow. There are some city-states whose CI do not
include 1 like New York, indicating statistical significance. 

## Problem 3
#### Loading and cleaning data for regression analysis
```{r}
bwt_df = read.csv("data/birthweight.csv")
bwt_df |> 
  summarise(across(everything(), ~sum(is.na(.x))))
bwt_df =
  bwt_df |> 
  mutate(
    babysex = 
        case_match(babysex,
            1 ~ "male",
            2 ~ "female"
        ),
    babysex = fct_infreq(babysex),
    frace = 
        case_match(frace,
            1 ~ "white",
            2 ~ "black", 
            3 ~ "asian", 
            4 ~ "puerto rican", 
            8 ~ "other"),
    frace = fct_infreq(frace),
    mrace = 
        case_match(mrace,
            1 ~ "white",
            2 ~ "black", 
            3 ~ "asian", 
            4 ~ "puerto rican",
            8 ~ "other"),
    mrace = fct_infreq(mrace),
    malform = as.logical(malform))
```
There do not appear to be any NA values.

#### Regression model for `bwt`
There's a lot of possible predictors for birthweight in this data, so `lasso` might be a good indicator of which predictors should be included.
```{r}
set.seed(123)
x = model.matrix(bwt ~ ., bwt_df)[,-1]
y = bwt_df |> pull(bwt)
lambda = 10^(seq(-2, 2.75, 0.1))
lasso_fit =
  glmnet(x, y, lambda = lambda)
lasso_cv =
  cv.glmnet(x, y, lambda = lambda)
lambda_opt = lasso_cv[["lambda.min"]]
lasso_fit = 
  glmnet(x, y, lambda = lambda_opt)
lasso_fit |> broom::tidy()
```

However, it was noted in lecture that lasso doesn't do much good on the full dataset since there
are so many points. Intuitively though, I would hypothesize that `mrace` and `malform` are good predictors
for birthweight. For one, it is well-established that race affects pregnancy-related outcomes and the presence
of malformations often mean the child has not developed as expected, which then usually means they're 
underweight. I also included `gaweeks` since I assumed that babies with longer gestation periods have
more time to developed, a smilar reasoning to inclusion of `malform`. 
```{r}
lm1 = lm(bwt ~  malform + mrace + gaweeks, data = bwt_df)
bwt_df |> 
  modelr::add_residuals(lm1) |> 
  modelr::add_predictions(lm1) |> 
  ggplot(aes(x = pred, y = resid)) +
  geom_point()
```

#### Model comparison

```{r}
lm2 = lm(bwt ~ blength + gaweeks, data = bwt_df)
lm3 = lm(bwt ~ bhead*blength*babysex, data = bwt_df)
```

```{r}
cv_bwt = 
  crossv_mc(bwt_df, 100)
cv_bwt =
  cv_bwt |> 
  mutate(
    model1 = map(cv_bwt$train, \(df) lm(bwt ~ malform + mrace + gaweeks, data = bwt_df)),
    model2 = map(cv_bwt$train, \(df) lm(bwt ~ blength + gaweeks, data = bwt_df)),
    model3 = map(cv_bwt$train, \(df) lm(bwt ~ bhead*blength*babysex, data = bwt_df))
  ) |> 
  mutate(
    rmse1 = map2_dbl(model1, test, \(mod, df) rmse(model = mod, data = df)),
    rmse2 = map2_dbl(model2, test, \(mod, df) rmse(model = mod, data = df)),
    rmse3 = map2_dbl(model3, test, \(mod, df) rmse(model = mod, data = df))
  )
cv_bwt |> 
  select(starts_with("rmse")) |> 
  pivot_longer(
    everything(),
    names_to = "model", 
    values_to = "rmse",
    names_prefix = "rmse_") |> 
  mutate(model = fct_inorder(model)) |> 
  ggplot(aes(x = model, y = rmse)) + geom_violin()
```

Based on the RMSEs of the three models, mine definitely does not do as well as the other two since
it has a noticeably higher RMSE. The model that includes the 3-way interaction seems to do the best,
so maybe including an interaction between `malform` and `gaweeks` would improve my model. Additionally,
the two models I compared mine to are purely related to the newborn, and does not take into account 
attributes of the mother, which I still believe plays a role. 



